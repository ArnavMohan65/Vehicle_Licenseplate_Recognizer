{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'51f17812'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [['5', (15.0, 15.0, 48.0, 68.0)], ['2', (252.0, 24.0, 283.0, 73.0)], ['7', (158.0, 21.0, 188.0, 71.0)], \n",
    "        ['8', (188.0, 22.0, 217.0, 71.0)], ['1', (229.0, 23.0, 249.0, 73.0)], \n",
    "        ['1', (54.0, 19.0, 74.0, 69.0)], ['1', (133.0, 19.0, 154.0, 71.0)], ['f', (79.0, 18.0, 111.0, 69.0)]]\n",
    "\n",
    "# Sort the list based on x1 values\n",
    "sorted_data = sorted(data, key=lambda item: item[1][0])\n",
    "\n",
    "# Extract the sorted names\n",
    "sorted_names = [item[0] for item in sorted_data]\n",
    "\n",
    "# print(sorted_names)\n",
    "\n",
    "\"\".join(sorted_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "from typing import List\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# sys.path.append(os.path.abspath('Character-Time-series-Matching/yolov5'))\n",
    "sys.path.insert(0, 'Character-Time-series-Matching/yolov5/')\n",
    "from utils_lp.general_lp import non_max_suppression, scale_coords\n",
    "from models_lp.experimental_lp import attempt_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "Model Summary: 213 layers, 7039792 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Fusing layers... \n",
      "Model Summary: 181 layers, 1214114 parameters, 0 gradients, 3.5 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "class Detection:\n",
    "    def __init__(self, weights_path='.pt',size=(640,640),device='cpu',iou_thres=None,conf_thres=None):\n",
    "        # cwd = os.path.dirname(__file__)\n",
    "        self.device=device\n",
    "        self.char_model, self.names = self.load_model(weights_path)\n",
    "        self.size=size\n",
    "        \n",
    "        self.iou_thres=iou_thres\n",
    "        self.conf_thres=conf_thres\n",
    "\n",
    "    def detect(self, frame):\n",
    "        \n",
    "        results, resized_img = self.char_detection_yolo(frame)\n",
    "\n",
    "        return results, resized_img\n",
    "    \n",
    "    def preprocess_image(self, original_image):\n",
    "\n",
    "        resized_img = self.ResizeImg(original_image,size=self.size)\n",
    "        # resized_img = original_image.copy()\n",
    "        image = resized_img.copy()[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
    "        image = np.ascontiguousarray(image)\n",
    "\n",
    "        image = torch.from_numpy(image).to(self.device)\n",
    "        image = image.float()\n",
    "        image = image / 255.0\n",
    "        if image.ndimension() == 3:\n",
    "            image = image.unsqueeze(0)\n",
    "        return image, resized_img\n",
    "    \n",
    "    def char_detection_yolo(self, image, classes=None, \\\n",
    "                            agnostic_nms=True, max_det=1000):\n",
    "\n",
    "        img,resized_img = self.preprocess_image(image.copy())\n",
    "        # print(resized_img.shape, image.shape)\n",
    "        pred = self.char_model(img, augment=False)[0]\n",
    "        \n",
    "        detections = non_max_suppression(pred, conf_thres=self.conf_thres,\n",
    "                                            iou_thres=self.iou_thres,\n",
    "                                            classes=classes,\n",
    "                                            agnostic=agnostic_nms,\n",
    "                                            multi_label=True,\n",
    "                                            labels=(),\n",
    "                                            max_det=max_det)\n",
    "        results=[]\n",
    "        for i, det in enumerate(detections):\n",
    "            det[:, :4]=scale_coords(resized_img.shape,det[:, :4],image.shape).round()\n",
    "            det=det.tolist()\n",
    "            if len(det):\n",
    "                for *xyxy, conf, cls in det:\n",
    "                    # xc,yc,w_,h_=(xyxy[0]+xyxy[2])/2,(xyxy[1]+xyxy[3])/2,(xyxy[2]-xyxy[0]),(xyxy[3]-xyxy[1])\n",
    "                    result=[self.names[int(cls)], str(conf), (xyxy[0],xyxy[1],xyxy[2],xyxy[3])]\n",
    "                    results.append(result)\n",
    "        # print(results)\n",
    "        return results, resized_img\n",
    "        \n",
    "    def ResizeImg(self, img, size):\n",
    "        h1, w1, _ = img.shape\n",
    "        # print(h1, w1, _)\n",
    "        h, w = size\n",
    "        if w1 < h1 * (w / h):\n",
    "            # print(w1/h1)\n",
    "            img_rs = cv2.resize(img, (int(float(w1 / h1) * h), h))\n",
    "            mask = np.zeros((h, w - (int(float(w1 / h1) * h)), 3), np.uint8)\n",
    "            img = cv2.hconcat([img_rs, mask])\n",
    "            trans_x = int(w / 2) - int(int(float(w1 / h1) * h) / 2)\n",
    "            trans_y = 0\n",
    "            trans_m = np.float32([[1, 0, trans_x], [0, 1, trans_y]])\n",
    "            height, width = img.shape[:2]\n",
    "            img = cv2.warpAffine(img, trans_m, (width, height))\n",
    "            return img\n",
    "        else:\n",
    "            img_rs = cv2.resize(img, (w, int(float(h1 / w1) * w)))\n",
    "            mask = np.zeros((h - int(float(h1 / w1) * w), w, 3), np.uint8)\n",
    "            img = cv2.vconcat([img_rs, mask])\n",
    "            trans_x = 0\n",
    "            trans_y = int(h / 2) - int(int(float(h1 / w1) * w) / 2)\n",
    "            trans_m = np.float32([[1, 0, trans_x], [0, 1, trans_y]])\n",
    "            height, width = img.shape[:2]\n",
    "            img = cv2.warpAffine(img, trans_m, (width, height))\n",
    "            return img\n",
    "    def load_model(self,path, train = False):\n",
    "        # print(self.device)\n",
    "        model = attempt_load(path, map_location=self.device)  # load FP32 model\n",
    "        names = model.module.names if hasattr(model, 'module') else model.names  # get class names\n",
    "        if train:\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "        return model, names\n",
    "    def xyxytoxywh(self, x):\n",
    "        # Convert nx4 boxes from [x1, y1, x2, y2] to [x, y, w, h] where xy1=top-left, xy2=bottom-right\n",
    "        y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "        y[0] = (x[0] + x[2]) / 2  # x center\n",
    "        y[1] = (x[1] + x[3]) / 2  # y center\n",
    "        y[2] = x[2] - x[0]  # width\n",
    "        y[3] = x[3] - x[1]  # height\n",
    "        return y\n",
    "    \n",
    "\n",
    "\n",
    "def crop_with_argwhere(image):\n",
    "    # Mask of non-black pixels (assuming image has a single channel).\n",
    "    mask = image > 0\n",
    "    \n",
    "    # Coordinates of non-black pixels.\n",
    "    coords = np.argwhere(mask)\n",
    "    \n",
    "    # Bounding box of non-black pixels.\n",
    "    x0, y0 = coords.min(axis=0)\n",
    "    x1, y1 = coords.max(axis=0) + 1   # slices are exclusive at the top\n",
    "    \n",
    "    # Get the contents of the bounding box.\n",
    "    cropped = image[x0:x1, y0:y1]\n",
    "    return x0,y0,x1,y1\n",
    "\n",
    "def crop_rgb_with_argwhere(image):\n",
    "    # Split the RGB image into its individual channels (R, G, B).\n",
    "    r_channel, g_channel, b_channel = cv2.split(image)\n",
    "    \n",
    "    # Apply the cropping function to each channel.\n",
    "    x0,y0,x1,y1 = crop_with_argwhere(r_channel)\n",
    "    r_cropped = r_channel[x0:x1, y0:y1]\n",
    "    g_cropped = g_channel[x0:x1, y0:y1]\n",
    "    b_cropped = b_channel[x0:x1, y0:y1]\n",
    "\n",
    "    \n",
    "    # Merge the cropped channels back into an RGB image.\n",
    "    cropped_rgb_image = cv2.merge((r_cropped, g_cropped, b_cropped))\n",
    "    \n",
    "    return cropped_rgb_image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_opt():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--obj-weights', nargs='+', type=str, default='object.pt', help='model path or triton URL')\n",
    "    parser.add_argument('--char-weights', nargs='+', type=str, default='char.pt', help='model path or triton URL')\n",
    "    parser.add_argument('--source', type=str, default='Vietnamese_imgs', help='file/dir')\n",
    "    parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[256], help='inference size h,w')\n",
    "    parser.add_argument('--conf-thres', type=float, default=0.25, help='confidence threshold')\n",
    "    parser.add_argument('--iou-thres', type=float, default=0.5, help='NMS IoU threshold')\n",
    "    parser.add_argument('--max-det', type=int, default=1000, help='maximum detections per image')\n",
    "    parser.add_argument('--device', default='cpu', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
    "    opt = parser.parse_args()\n",
    "    opt.imgsz *= 2 if len(opt.imgsz) == 1 else 1  # expand\n",
    "\n",
    "    return opt\n",
    "\n",
    "\n",
    "# opt = parse_opt()\n",
    "imgsz = (256,256)\n",
    "obj_weights = 'Character-Time-series-Matching/Vietnamese/object.pt'\n",
    "char_weights = 'Character-Time-series-Matching/Vietnamese/char.pt'\n",
    "device = 'cpu'\n",
    "iou_thres = 0.5\n",
    "conf_thres = 0.25\n",
    "\n",
    "    \n",
    "obj_model=Detection(size=imgsz,weights_path=obj_weights,device=device,iou_thres=iou_thres,conf_thres=conf_thres)\n",
    "char_model=Detection(size=imgsz,weights_path=char_weights,device=device,iou_thres=iou_thres,conf_thres=conf_thres)\n",
    "# path=opt.source\n",
    "char_model.size=(256,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models_lp.yolo_lp'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_name = \"models.yolo\"\n",
    "\".\".join([n+\"_lp\" for n in mod_name.split(\".\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test this on webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "count = 0\n",
    "\n",
    "# Create a named window (you can give it any name)\n",
    "cv2.namedWindow(\"frame\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "# Set the window size (width, height)\n",
    "cv2.resizeWindow(\"frame\", 1280, 720)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # object detection\n",
    "    results, resized_img=obj_model.detect(frame.copy())\n",
    "    cropped_image = None\n",
    "    x1,y1,x2,y2 = 0,0,0,0\n",
    "    for name,conf,box in results:\n",
    "        # if(name!='rectangle license plate' or name!='square license plate'):\n",
    "        #     frame=cv2.putText(frame, \"{}\".format(name), (int(box[0]), int(box[1])-3),\n",
    "        #                         cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "        #                         (255, 0, 255), 2)\n",
    "        #     frame = cv2.rectangle(frame, (int(box[0]),int(box[1])), (int(box[2]),int(box[3])), (0,0,255), 1)\n",
    "        if(name=='rectangle license plate' or name=='square license plate'):\n",
    "#             # Crop the image using the ROI coordinates\n",
    "            cropped_image = frame[int(box[1]):int(box[3]), int(box[0]):int(box[2])]\n",
    "            x1,y1,x2,y2 = int(box[0]),int(box[1]),int(box[2]),int(box[3])\n",
    "            # print(cropped_image.shape)\n",
    "\n",
    "\n",
    "    # character detection\n",
    "    if(cropped_image is not None and cropped_image.size > 0):\n",
    "        results2, resized_img2=char_model.detect(cropped_image.copy())\n",
    "        for name,conf,box in results2:\n",
    "            cropped_image=cv2.putText(cropped_image, \"{}\".format(name), (int(box[0]), int(box[1])-3),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                                    (255, 0, 255), 2)\n",
    "            cropped_image = cv2.rectangle(cropped_image, (int(box[0]),int(box[1])), (int(box[2]),int(box[3])), (0,0,255), 1)\n",
    "            \n",
    "        if(len(cropped_image)>0 and x1!=0 and y1!=0 and x2!=0 and y2!=0):\n",
    "            # pass\n",
    "            # resized_img[y1:y2, x1:x2] = cv2.resize(crop_rgb_with_argwhere(resized_img2),cropped_image.shape[:2][::-1])\n",
    "            frame[y1:y2, x1:x2] = cropped_image\n",
    "    \n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "    count += 1\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali/.local/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Model Summary: 305 layers, 446376 parameters, 0 gradients, 1.5 GFLOPS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n"
     ]
    }
   ],
   "source": [
    "#pytorch\n",
    "from concurrent.futures import thread\n",
    "from sqlalchemy import null\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import time\n",
    "from threading import Thread\n",
    "\n",
    "#other lib\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "sys.path.insert(0, \"face-recognition/yolov5_face/\")\n",
    "# sys.path.append(os.path.abspath('face-recognition'))\n",
    "from models.experimental import attempt_load_face\n",
    "from utils.datasets import letterbox\n",
    "from utils.general import non_max_suppression_face, check_img_size, scale_coords\n",
    "\n",
    "\n",
    "sys.path.insert(0, 'face-recognition')\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Get model detect\n",
    "## Case 1:\n",
    "# model = attempt_load(\"yolov5_face/yolov5s-face.pt\", map_location=device)\n",
    "\n",
    "## Case 2:\n",
    "model = attempt_load_face(\"face-recognition/yolov5_face/yolov5n-0.5.pt\", map_location=device)\n",
    "\n",
    "# Get model recognition\n",
    "## Case 1: \n",
    "\n",
    "from insightface.insight_face import iresnet100\n",
    "weight = torch.load(\"face-recognition/insightface/resnet100_backbone.pth\", map_location = device)\n",
    "model_emb = iresnet100()\n",
    "\n",
    "model_emb.load_state_dict(weight)\n",
    "model_emb.to(device)\n",
    "model_emb.eval()\n",
    "\n",
    "face_preprocess = transforms.Compose([\n",
    "                                    transforms.ToTensor(), # input PIL => (3,56,56), /255.0\n",
    "                                    transforms.Resize((112, 112)),\n",
    "                                    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "                                    ])\n",
    "\n",
    "isThread = True\n",
    "score = 0\n",
    "name = null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python face-recognition/train.py --is-add-user=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignore the specific warning by category\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful\n",
      "successful\n",
      "successful\n",
      "successful\n",
      "successful\n",
      "successful\n",
      "successful\n",
      "successful\n",
      "successful\n",
      "successful\n",
      "successful\n",
      "successful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful\n"
     ]
    }
   ],
   "source": [
    "def resize_image(img0, img_size):\n",
    "    h0, w0 = img0.shape[:2]  # orig hw\n",
    "    r = img_size / max(h0, w0)  # resize image to img_size\n",
    "\n",
    "    if r != 1:  # always resize down, only resize up if training with augmentation\n",
    "        interp = cv2.INTER_AREA if r < 1  else cv2.INTER_LINEAR\n",
    "        img0 = cv2.resize(img0, (int(w0 * r), int(h0 * r)), interpolation=interp)\n",
    "\n",
    "    imgsz = check_img_size(img_size, s=model.stride.max())  # check img_size\n",
    "    img = letterbox(img0, new_shape=imgsz)[0]\n",
    "\n",
    "    # Convert\n",
    "    img = img[:, :, ::-1].transpose(2, 0, 1).copy()  # BGR to RGB, to 3x416x416\n",
    "\n",
    "    img = torch.from_numpy(img).to(device)\n",
    "    img = img.float()  # uint8 to fp16/32\n",
    "    img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "    \n",
    "    return img\n",
    "\n",
    "def scale_coords_landmarks(img1_shape, coords, img0_shape, ratio_pad=None):\n",
    "    # Rescale coords (xyxy) from img1_shape to img0_shape\n",
    "    if ratio_pad is None:  # calculate from img0_shape\n",
    "        gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # gain  = old / new\n",
    "        pad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2  # wh padding\n",
    "    else:\n",
    "        gain = ratio_pad[0][0]\n",
    "        pad = ratio_pad[1]\n",
    "\n",
    "    coords[:, [0, 2, 4, 6, 8]] -= pad[0]  # x padding\n",
    "    coords[:, [1, 3, 5, 7, 9]] -= pad[1]  # y padding\n",
    "    coords[:, :10] /= gain\n",
    "    #clip_coords(coords, img0_shape)\n",
    "    coords[:, 0].clamp_(0, img0_shape[1])  # x1\n",
    "    coords[:, 1].clamp_(0, img0_shape[0])  # y1\n",
    "    coords[:, 2].clamp_(0, img0_shape[1])  # x2\n",
    "    coords[:, 3].clamp_(0, img0_shape[0])  # y2\n",
    "    coords[:, 4].clamp_(0, img0_shape[1])  # x3\n",
    "    coords[:, 5].clamp_(0, img0_shape[0])  # y3\n",
    "    coords[:, 6].clamp_(0, img0_shape[1])  # x4\n",
    "    coords[:, 7].clamp_(0, img0_shape[0])  # y4\n",
    "    coords[:, 8].clamp_(0, img0_shape[1])  # x5\n",
    "    coords[:, 9].clamp_(0, img0_shape[0])  # y5\n",
    "    return coords\n",
    "\n",
    "def get_face(input_image):\n",
    "    # Parameters\n",
    "    size_convert = 128\n",
    "    conf_thres = 0.4\n",
    "    iou_thres = 0.5\n",
    "    \n",
    "    # Resize image\n",
    "    img = resize_image(input_image.copy(), size_convert)\n",
    "\n",
    "    # Via yolov5-face\n",
    "    with torch.no_grad():\n",
    "        pred = model(img[None, :])[0]\n",
    "\n",
    "    # Apply NMS\n",
    "    det = non_max_suppression_face(pred, conf_thres, iou_thres)[0]\n",
    "    bboxs = np.int32(scale_coords(img.shape[1:], det[:, :4], input_image.shape).round().cpu().numpy())\n",
    "    \n",
    "    landmarks = np.int32(scale_coords_landmarks(img.shape[1:], det[:, 5:15], input_image.shape).round().cpu().numpy())    \n",
    "    \n",
    "    return bboxs, landmarks\n",
    "\n",
    "def get_feature(face_image, training = True): \n",
    "    # Convert to RGB\n",
    "    face_image = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Preprocessing image BGR\n",
    "    face_image = face_preprocess(face_image).to(device)\n",
    "    \n",
    "    # Via model to get feature\n",
    "    with torch.no_grad():\n",
    "        if training:\n",
    "            emb_img_face = model_emb(face_image[None, :])[0].cpu().numpy()\n",
    "        else:\n",
    "            emb_img_face = model_emb(face_image[None, :]).cpu().numpy()\n",
    "    \n",
    "    # Convert to array\n",
    "    images_emb = emb_img_face/np.linalg.norm(emb_img_face)\n",
    "    return images_emb\n",
    "\n",
    "def read_features(root_fearure_path = \"face-recognition/static/feature/face_features.npz\"):\n",
    "    data = np.load(root_fearure_path, allow_pickle=True)\n",
    "    images_name = data[\"arr1\"]\n",
    "    images_emb = data[\"arr2\"]\n",
    "    \n",
    "    return images_name, images_emb\n",
    "\n",
    "def recognition(face_image):\n",
    "    global isThread, score, name\n",
    "    \n",
    "    # Get feature from face\n",
    "    query_emb = (get_feature(face_image, training=False))\n",
    "    \n",
    "    # Read features\n",
    "    images_names, images_embs = read_features()   \n",
    "\n",
    "    scores = (query_emb @ images_embs.T)[0]\n",
    "\n",
    "    id_min = np.argmax(scores)\n",
    "    score = scores[id_min]\n",
    "    name = images_names[id_min]\n",
    "    isThread = True\n",
    "    print(\"successful\")\n",
    "\n",
    "\n",
    "\n",
    "global isThread, score, name\n",
    "\n",
    "# Open camera \n",
    "cap = cv2.VideoCapture(0)\n",
    "start = time.time_ns()\n",
    "frame_count = 0\n",
    "fps = -1\n",
    "\n",
    "# Create a named window (you can give it any name)\n",
    "# cv2.namedWindow(\"frame\", cv2.WINDOW_NORMAL)\n",
    "# cv2.resizeWindow(\"frame\", 1280, 720)\n",
    "\n",
    "# Save video\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "\n",
    "size = (frame_width, frame_height)\n",
    "video = cv2.VideoWriter('face-recognition/static/results/alfpr-demo.mp4',cv2.VideoWriter_fourcc(*'mp4v'), 6, size)\n",
    "\n",
    "# Read until video is completed\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    _, frame = cap.read()\n",
    "    \n",
    "    # Get faces\n",
    "    bboxs, landmarks = get_face(frame)\n",
    "    h, w, c = frame.shape\n",
    "    \n",
    "    tl = 1 or round(0.002 * (h + w) / 2) + 1  # line/font thickness\n",
    "    clors = [(255,0,0),(0,255,0),(0,0,255),(255,255,0),(0,255,255)]\n",
    "    \n",
    "    # Get boxs\n",
    "    for i in range(len(bboxs)):\n",
    "        # Get location face\n",
    "        x1, y1, x2, y2 = bboxs[i]\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 146, 230), 2)\n",
    "        \n",
    "        # Landmarks\n",
    "        for x in range(5):\n",
    "            point_x = int(landmarks[i][2 * x])\n",
    "            point_y = int(landmarks[i][2 * x + 1])\n",
    "            cv2.circle(frame, (point_x, point_y), tl+1, clors[x], -1)\n",
    "        \n",
    "        # Get face from location\n",
    "        if isThread == True:\n",
    "            isThread = False\n",
    "            \n",
    "            # Recognition\n",
    "            face_image = frame[y1:y2, x1:x2]\n",
    "            thread = Thread(target=recognition, args=(face_image,))\n",
    "            thread.start()\n",
    "    \n",
    "        if name == null:\n",
    "            continue\n",
    "        else:\n",
    "            if score < 0.25:\n",
    "                caption= \"UN_KNOWN\"\n",
    "            else:\n",
    "                caption = f\"{name.split('_')[0].upper()}:{score:.2f}\"\n",
    "\n",
    "            t_size = cv2.getTextSize(caption, cv2.FONT_HERSHEY_PLAIN, 2, 2)[0]\n",
    "            \n",
    "            cv2.rectangle(frame, (x1, y1), (x1 + t_size[0], y1 + t_size[1]), (0, 146, 230), -1)\n",
    "            cv2.putText(frame, caption, (x1, y1 + t_size[1]), cv2.FONT_HERSHEY_PLAIN, 2, [255, 255, 255], 2)       \n",
    "        \n",
    "    \n",
    "    # object detection\n",
    "    results, resized_img=obj_model.detect(frame.copy())\n",
    "    cropped_image = None\n",
    "    x1,y1,x2,y2 = 0,0,0,0\n",
    "    for names,conf,box in results:\n",
    "        # if(names!='rectangle license plate' or names!='square license plate'):\n",
    "        #     frame=cv2.putText(frame, \"{}\".format(names), (int(box[0]), int(box[1])-3),\n",
    "        #                         cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "        #                         (255, 0, 255), 2)\n",
    "        #     frame = cv2.rectangle(frame, (int(box[0]),int(box[1])), (int(box[2]),int(box[3])), (0,0,255), 1)\n",
    "        if(names=='rectangle license plate' or names=='square license plate'):\n",
    "#             # Crop the image using the ROI coordinates\n",
    "            cropped_image = frame[int(box[1]):int(box[3]), int(box[0]):int(box[2])]\n",
    "            x1,y1,x2,y2 = int(box[0]),int(box[1]),int(box[2]),int(box[3])\n",
    "            # print(cropped_image.shape)\n",
    "\n",
    "\n",
    "    # character detection\n",
    "    if(cropped_image is not None and cropped_image.size > 0):\n",
    "        results2, resized_img2=char_model.detect(cropped_image.copy())\n",
    "        for names,conf,box in results2:\n",
    "            cropped_image=cv2.putText(cropped_image, \"{}\".format(names), (int(box[0]), int(box[1])-3),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                                    (255, 0, 255), 2)\n",
    "            cropped_image = cv2.rectangle(cropped_image, (int(box[0]),int(box[1])), (int(box[2]),int(box[3])), (0,0,255), 1)\n",
    "            \n",
    "        if(len(cropped_image)>0 and x1!=0 and y1!=0 and x2!=0 and y2!=0):\n",
    "            # pass\n",
    "            # resized_img[y1:y2, x1:x2] = cv2.resize(crop_rgb_with_argwhere(resized_img2),cropped_image.shape[:2][::-1])\n",
    "            frame[y1:y2, x1:x2] = cropped_image\n",
    "\n",
    "    # Count fps \n",
    "    frame_count += 1\n",
    "    \n",
    "    if frame_count >= 30:\n",
    "        end = time.time_ns()\n",
    "        fps = 1e9 * frame_count / (end - start)\n",
    "        frame_count = 0\n",
    "        start = time.time_ns()\n",
    "\n",
    "    if fps > 0:\n",
    "        fps_label = \"FPS: %.2f\" % fps\n",
    "        cv2.putText(frame, fps_label, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    \n",
    "    \n",
    "    video.write(frame)\n",
    "    cv2.imshow(\"Face Recognition\", frame)\n",
    "    \n",
    "    # Press Q on keyboard to  exit\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break  \n",
    "\n",
    "video.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "/home/ali/anaconda3/envs/facerec/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "torch.Size([1, 16])\n",
      "/home/ali/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "successful\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "successful\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "successful\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "successful\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "successful\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "successful\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "successful\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "successful\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "successful\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n",
      "successful\n"
     ]
    }
   ],
   "source": [
    "!python face-recognition/recognize.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facerec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
